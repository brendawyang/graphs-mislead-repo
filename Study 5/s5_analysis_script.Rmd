---
title: "Data Analysis for Study 5"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    toc_float: FALSE
---


# Study 5 Introduction

This Markdown documents the process to analyze the data for Study 5 which looked at the difference between those who received and those who did ./not receive a warning at the beginning of the study for a new population: experts. Experts were PhD students from humanities and quantitative fields. 

Data was collected using qualtrics between October 23 and November 25th, 2018 using Amazon's Mechanical Turk for ditribution and Qualtrics as a survey platform. 

This HTML was last knitted on: `r Sys.time()`

## Set Up

### Packages and Libraries

You must run this section before you can run any other chunks.


```{r packages, echo = FALSE}
#Make sure all packages are installed

list.of.packages <- c("readr", "tidyr", "dplyr", "magrittr", "psych", "stringr", "effsize", "shiny", "readr", "lme4", "lmerTest", "kableExtra", "knitr", "ggplot2", "ggthemes", "cowplot", "pwr", "psych", "emmeans")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```


```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

#turn off scientific notation
options(scipen=999)
```


```{r libraries}
#Load all libraries
library(readr)
library(tidyr)
library(dplyr)
library(magrittr)
library(effsize)
library(lme4) # for mixed effects models
library(lmerTest)
library(kableExtra)
library(knitr)#kable
library(pwr)
library(emmeans)

##Raincloud Plot Libraries
source("../R_rainclouds.R")
library(cowplot)
library(ggthemes)
library(ggplot2)

```

### ggplot themes
```{r raincloud plot}
#raincloud plot theme
raincloud_theme <- theme(
  text = element_text(size = 10, color = "black"),
  axis.title.x = element_text(size = 16, color = "black"),
  axis.title.y = element_text(size = 16, color = "black", margin = margin(r = 20)),
  axis.text = element_text(size = 14, color = "black"),
  #axis.text.x = element_text(color = "black"),
  #axis.text.y = element_text(color = "black"),
  legend.title=element_blank(),
  legend.text=element_text(size=16),
  legend.position = "right",
  plot.title = element_text(lineheight=.8, face="bold", size = 16),
  panel.border = element_blank(),
  panel.grid.minor = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.major.y = element_line(colour = 'light gray', size=0.5, linetype='solid'),
  panel.background = element_blank(),
  axis.line = element_blank(), 
  axis.ticks.y = element_blank(),
  strip.text = element_text(size=15, margin = margin(.3,0,.3,0, "cm")),
  strip.background = element_rect(fill="white")
  )

#study5 theme
study5_theme <- theme(
  text = element_text(size = 10),
  axis.title.x = element_text(size = 14),
  axis.title.y = element_text(size = 14, margin = margin(r = 20)),
  axis.text = element_text(size = 12),
  legend.title=element_blank(),
  legend.text=element_text(size=14),
  legend.position = "right",
  plot.title = element_text(lineheight=.8, face="bold", size = 14),
  panel.border = element_blank(),
  panel.grid.minor = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.major.y = element_line(colour = 'light gray', size=0.5, linetype='solid'),
  panel.background = element_blank(),
  axis.line = element_blank(), 
  axis.ticks.y = element_blank())

```


### Data Import

```{r data import}

rating_df <- read_csv("data/clean/s5_rating_df.csv")
rating_df$field <- tolower(rating_df$field)

demographic_df <- read_csv("data/clean/s5_demographic_df.csv")
demographic_df$field <- tolower(demographic_df$field) 
# the noticed column of this dataset is not clean

graphliteracy_df <- read_csv("data/clean/s5_graphliteracy_df.csv")
graphliteracy_df$field <- tolower(graphliteracy_df$field)

debriefing_df <- read_csv("data/clean/s5_debriefing_df.csv")
debriefing_df$field <- tolower(debriefing_df$field)

timing_df <- read_csv("data/clean/s5_timing_df.csv")
timing_df$field <- tolower(timing_df$field)

```


### Exclusions

You can exclude subjects who did not get the correct answer in the exercise by changing Exclude_Exercise_Check (line 97) to TRUE. The next time you run all the code, these participants will be excluded. 

```{r}
Exclude_Exercise_Check <- FALSE


participants_excluded <- demographic_df %>% 
  filter(check == "wrong") %>% 
  group_by(check) %>% 
  summarise(n = n())

if(Exclude_Exercise_Check == TRUE){rating_df <- rating_df %>% 
  filter(check == "right" | subject_condition == "no warning")
} else{rating_df <- rating_df}

if(Exclude_Exercise_Check == TRUE){demographic_df <- demographic_df %>% 
  filter(check == "right"| subject_condition == "no warning")
} else{demographic_df <- demographic_df}

```


For this report, **`r if(Exclude_Exercise_Check == FALSE){paste("no")} else{paste(participants_excluded$n)}`** participants are being excluded from analysis. 

#### Spontaneously noticed manipulation

To exclude participants who spontaneously noticed the manipulation, set the following chunk to TRUE.

```{r}
Exclude_Notice <- FALSE

```


```{r import notice_df}

# import
notice_df <- read_csv("data/clean/s5_notice_df.csv")

# merge
rating_df <- left_join(rating_df, notice_df)
demographic_df <- left_join(demographic_df, notice_df)
#graphliteracy_df <- left_join(graphliteracy_df, notice_df)

```


```{r}

# How many people are we potentially excluding?
notice_df %>% 
  filter(noticed == 1) %>% 
  group_by(noticed) %>% 
  summarise(n = n())

# How do the numbers break down by field?
notice_df %>% 
  filter(noticed == 1) %>% 
  group_by(field) %>% 
  summarise(n = n())

# Do the exclusions from datasets
if(Exclude_Notice == TRUE){rating_df <- rating_df %>% 
  filter(noticed != 1)
} else{rating_df <- rating_df}

if(Exclude_Notice == TRUE){demographic_df <- demographic_df %>% 
  filter(noticed != 1)
} else{demographic_df <- demographic_df}

if(Exclude_Notice == TRUE){graphliteracy_df <- graphliteracy_df %>% 
  filter(noticed != 1)
} else{graphliteracy_df <- graphliteracy_df}

```

```{r QA, eval=FALSE, include=FALSE}

nrow(rating_df)

```


```{r addtional data frames, eval = TRUE}
#Create data frames that will be used throughout

#Calculate subject means by condition 
##(mean rating for truncated and mean rating for non-truncated graphs)
subject_mean_df <- rating_df %>% 
  group_by(participantID, field, subject_condition, graph_condition) %>% 
  summarise(subject_mean_rating = mean(rating, na.rm = TRUE)) %>% 
  arrange(participantID)


#Calculate subject difference rating
##(non-truncated mean rating - truncated mean rating)
subject_difference_df <- subject_mean_df %>% 
  spread(graph_condition, subject_mean_rating) %>% 
  mutate(difference =  truncated - control)


#Calculate subject overall graph literacy score
##(sum of graph literacy items)
subject_graphliteracy_df <- graphliteracy_df %>%
  group_by(participantID, field, subject_condition) %>%
  summarise(graphliteracy_sum_rating = sum(rating)) 


#Create mixed effects df
##(combination of ratings, overall graph literacy scores and some demographic questions --- education, gender and age)
models_df <- full_join(full_join(subject_graphliteracy_df, demographic_df[, c("participantID", "dem_ed", "dem_gender", "dem_age")]), rating_df)


##Create mixed effects difference df
##(df with subject truncation effect scores (difference between truncated and control) + all demographics and graph literacy scores) 
models_difference_df <- left_join(left_join(subject_difference_df,subject_graphliteracy_df), demographic_df[, c("participantID", "dem_ed", "dem_gender", "dem_age")])

#Create trimmed timing df
##(trims anything past 2 standard deviations from each participant's individual mean for each condition)
trimmed_timing_df <- timing_df %>% 
  group_by(participantID, subject_condition, graph_condition) %>% 
  mutate(avg = mean(time), stdev = sd(time)) %>%
  filter(time <= 2*stdev+avg) %>%
  as.data.frame()

```



--------------

## Methods

### Participants


```{r general participant information}
n <- demographic_df %>%
  group_by(field) %>% 
  summarise(n = n())
```


```{r general participant information gender}
gender <- demographic_df %>% 
  group_by(field, dem_gender) %>% 
  summarise(n = n())

gender
  
```


```{r general participant information age}
age <- demographic_df %>% 
  group_by(field) %>% 
  summarise(mean_age = mean(dem_age,na.rm = TRUE), sd_age = sd(dem_age,na.rm = TRUE), 
            median_age = median(dem_age, na.rm = TRUE), min_age= min(dem_age, na.rm = TRUE), 
            max_age = max(dem_age, na.rm = TRUE), range_age = max(dem_age, na.rm = TRUE)-min(dem_age, na.rm = TRUE))

age

#Note na.rm is TRUE because some participants (n=4) did not give their age information. 
# demographic_df %>%
#   filter(is.na(dem_age))
```

`r text_spec(n[n$field == "quantitative",]$n, bold = T)` PhD students (`r text_spec(gender[gender$field == "quantitative" & gender$dem_gender == "female",]$n, bold = T)` women, `r text_spec(gender[gender$field == "quantitative" & gender$dem_gender == "non-binary",]$n, bold = T)` non-binary gender; M~age~ = `r text_spec(round(age[age$field == "quantitative",]$mean_age, 2), bold = T)` years, SD~age~ = `r text_spec(round(age[age$field == "quantitative",]$sd_age, 2), bold = T)`)  in the departments of Statistics, Psychology, and Economics and `r text_spec(n[n$field == "humanities",]$n, bold = T)`  (`r text_spec(gender[gender$field == "humanities" & gender$dem_gender == "female",]$n, bold = T)` women, `r text_spec(gender[gender$field == "humanities" & gender$dem_gender == "non-binary",]$n, bold = T)` non-binary gender; M~age~ = `r text_spec(round(age[age$field == "humanities",]$mean_age, 2), bold = T)` years, SD~age~ = `r text_spec(round(age[age$field == "humanities",]$sd_age, 2), bold = T)`) in the departments of History and English participated in Study 5. 


Using publicly available email addresses, and after consulting with the universities’ review boards, we recruited PhD students from the top programs in these fields. A list of schools we recruited from is available in our Supplemental Information. 

```{r summary statistics for graph literacy}
#Cronbach's Alpha
library(psych)

alpha <- graphliteracy_df %>% 
  spread(graphliteracy_question, rating) %>% 
  select(-group) %>% 
  psych::alpha()

#Data Summary
subject_graphliteracy_summary <- subject_graphliteracy_df %>%
  ungroup() %>% 
  group_by(field) %>% 
  summarise(n = n(), 
            mean = mean(graphliteracy_sum_rating, na.rm = TRUE), 
            sd = sd(graphliteracy_sum_rating, na.rm = TRUE), 
            median = median(graphliteracy_sum_rating, na.rm = TRUE),
            min= min(graphliteracy_sum_rating, na.rm = TRUE), 
            max = max(graphliteracy_sum_rating, na.rm = TRUE), 
            range=max(graphliteracy_sum_rating, na.rm = TRUE)-min(graphliteracy_sum_rating, na.rm = TRUE)) %>% 
  mutate(alpha = round(alpha$total$std.alpha,2))

subject_graphliteracy_summary

#Note na.rm is TRUE because one participant (n=1) did not give their graph literacy information. 
# subject_graphliteracy_df %>% 
#   filter(is.na(graphliteracy_sum_rating))
```


Both reports of formal statistical training and the graph literacy scale scores support our assumptions about the two populations: Participants from the quantitative sample scored higher on the graph literacy measure (M = `r text_spec(round(subject_graphliteracy_summary[subject_graphliteracy_summary$field == "quantitative",]$mean,2), bold = T)`, SD~graph literacy~= `r text_spec(round(subject_graphliteracy_summary[subject_graphliteracy_summary$field == "quantitative",]$sd,2), bold = T)`) than did  participants from the humanities (M = `r text_spec(round(subject_graphliteracy_summary[subject_graphliteracy_summary$field == "humanities",]$mean, 2), bold = T)`, SD~graph literacy~= `r text_spec(round(subject_graphliteracy_summary[subject_graphliteracy_summary$field == "humanities",]$sd,2), bold = T)`).


```{r}
t_test <- t.test(graphliteracy_sum_rating ~ field, subject_graphliteracy_df, paired = FALSE, var.equal = TRUE)

t_test
```

```{r effect size for graph literacy}
cohen_d <- effsize::cohen.d(graphliteracy_sum_rating ~ field, subject_graphliteracy_df, paired = FALSE, var.equal = TRUE)

cohen_d
```


The difference in graph literacy between these samples is statistically significant with a large effect size: t(`r text_spec(round(t_test$parameter,2), bold = T)`) = `r text_spec(abs(round(t_test$statistic,2)), bold = T)`, p < `r text_spec(format(round(t_test$p.value,3), nsmall= 2), bold = T)`, d = `r text_spec(abs(round(cohen_d$estimate,2)), bold = T)`




```{r}
stat_training <- demographic_df %>% 
  group_by(field, dem_stat_formal) %>% 
  summarise(n = n()) %>%
  mutate(percentage = round(n / sum(n)*100, 2))
  
```


We use a sample of PhD students from the fields of psychology, statistics, and economics with the rationale that these fields require that students regularly read and produce graphs. `r text_spec(stat_training[stat_training$field == "quantitative" & stat_training$dem_stat_formal == 1,]$percentage, bold = T)`% of the participants from this sample reported having formal statistical training. We also used a sample of PhD students from English and History departments, assuming that these students have a lower exposure to graphs. In fitting with our recruitment strategy, only `r text_spec(stat_training[stat_training$field == "humanities" & stat_training$dem_stat_formal == 1,]$percentage, bold = T)`% of the participants from this sample reported having formal statistical training. 

```{r}
chisq.test(demographic_df$field, demographic_df$dem_stat_formal, correct=FALSE)
```


### Exercise Check

```{r}
exercise_check <- rating_df %>% 
  filter(subject_condition == "warning") %>% 
  group_by(participantID, check, field) %>% 
  summarise(n()) %>% 
  group_by(field, check) %>% 
  summarise(n = n()) %>% 
  mutate(percentage = n / sum(n)*100)
```

`r text_spec(round(mean(exercise_check[exercise_check$check == "right",]$percentage),2), bold = T)`% of all participants in the warning condition answered the training exercise question correctly.

`r text_spec(round(exercise_check[exercise_check$field == "humanities" & exercise_check$check == "right",]$percentage,2), bold = T)`% of the humanities participants in the warning condition answered the training exercise question correctly.

`r text_spec(round(exercise_check[exercise_check$field == "quantitative" & exercise_check$check == "right",]$percentage,2), bold = T)`% of the quantitative participants in the warning condition answered the training exercise question correctly.


For this report, `r if(Exclude_Exercise_Check == FALSE){paste("no")} else{paste(participants_excluded$n)}` participants are being excluded from analysis. If you want to see results when participants who got the exercise wrong are excluded/included, you can go to the section called Exclusions (at the top of this file) and change **Exclude_Exercise_Check <- TRUE**

---------------------------------

## Results

### The truncation effect

We first replicated the truncation effect found in Studies 1 through 4. 

```{r subject_mean_main}
subject_mean_main <- subject_mean_df %>% 
group_by(graph_condition) %>% 
summarise(n = n(), mean = mean(subject_mean_rating), 
            sd = sd(subject_mean_rating), 
            median = median(subject_mean_rating),
            min= min(subject_mean_rating), max = max(subject_mean_rating), 
            range =max(subject_mean_rating)-min(subject_mean_rating))

subject_mean_main
```

We found that average ratings of differences for truncated graphs was higher than ratings for control graphs:  M~control~ = `r text_spec(round(subject_mean_main$mean[subject_mean_main$graph_condition=="control"],2), bold =T)`, SD = `r text_spec(round(subject_mean_main$sd[subject_mean_main$graph_condition=="control"],2), bold =T)`; M~truncated~ = `r text_spec(round(subject_mean_main$mean[subject_mean_main$graph_condition=="truncated"],2), bold =T)`, SD = `r text_spec(round(subject_mean_main$sd[subject_mean_main$graph_condition=="truncated"],2), bold =T)`. 

```{r}
subject_mean_main_df <- subject_mean_df %>% 
  group_by(participantID, subject_condition, graph_condition) %>% 
  summarise(subject_mean_rating = mean(subject_mean_rating))
```

```{r effect size for main effect}
cohen_d <- effsize::cohen.d(subject_mean_main_df$subject_mean_rating, subject_mean_main_df$graph_condition, paired = TRUE, na.rm = TRUE)

cohen_d
```

```{r t test for main effect}
t_test <- t.test(subject_mean_rating ~ graph_condition, subject_mean_main_df, paired = TRUE)

t_test
```


This main effect was statistically significant: t(`r text_spec(round(t_test$parameter,2), bold = T)`) = `r text_spec(abs(round(t_test$statistic,2)), bold = T)`, p < `r text_spec(format(t_test$p.value,scientific = T), bold = T)`, 95% CI of difference = [0.37, 0.50],  d = `r text_spec(abs(round(cohen_d$estimate,2)), bold = T)`. . 

```{r truncation effect direction}
truncation_effect_direction <- subject_difference_df %>% 
  mutate(direction = ifelse(difference > 0, "expected", "unexpected")) %>%
  group_by(field, direction) %>% 
  summarise(n= n()) %>% 
  mutate(percentage = n / sum(n)*100)

truncation_effect_direction           
```

As before, most participants from both fields showed an overall truncation effect:`r text_spec(round(truncation_effect_direction[truncation_effect_direction$direction == "expected" & truncation_effect_direction$field == "humanities",]$percentage, 2), bold = T)`% of participants from the humanities group, and `r text_spec(round(truncation_effect_direction[truncation_effect_direction$direction == "expected" & truncation_effect_direction$field == "quantitative",]$percentage, 2), bold = T)`% in the quantitative group. 

### 2x2x2 mixed effects linear model

Next, we computed a linear mixed-effects model with graph type (0 = control, 1 = truncated), warning condition (0 = no warning, 1 = warning), and field (0 = humanities, 1 = quantitative) as binary fixed factors, with the first listed condition as the reference group for each factor. The outcome variable was graph ratings, and participant was modeled as a random effect. Table S4 shows estimates for this model in full. 

```{r mixed effects model full}

model_full<- lmer(rating ~ subject_condition*graph_condition*field + (1 | participantID), models_df)
summary_model_full <- summary(model_full)
summary_model_full <- as.data.frame(summary_model_full$coefficients)

```

Table S4

```{r}

##Confidence Intervals
ci_model_full <- confint(model_full, "beta_", level = 0.95, method = c("boot"), nsim = 1000,  seed = 833, boot.type = c("perc"), FUN=NULL, quiet=FALSE, oldNames = TRUE)
ci_model_full <- as.data.frame(ci_model_full)


## Create and Clean Table
thetable_full <- merge(summary_model_full, ci_model_full, by = "row.names")
numVars <- sapply(thetable_full, is.numeric) 
thetable_full[numVars] <- lapply(thetable_full[numVars], round, digits = 3) 

thetable_full %<>% 
  mutate("95% CI of Estimate" =  paste("[", `2.5 %`, ",", `97.5 %`, "]", sep = "")) %>% select(Row.names, Estimate, `95% CI of Estimate`, `Std. Error`, `t value`, `Pr(>|t|)`)

thetable_full
```

We found statistically significant simple effects of graph type (t = 20.3, p < .0001), warning condition (t = 3.31, p = .001), and field (t = 2.19, p = .02). We also found statistically significant interactions between graph type and warning condition (t = 8.59, p < .0001), graph type and field (t = 2.97, p = .002), and field and warning condition (t = 2.47, p = .01). The 3-way interaction between field, warning condition, and graph type was not statistically significant (p = .27). We ran follow-up models to clarify these interactions, described below. To preview, Figure 8 summarizes our results: we replicated the finding that an explanatory warning reduces but does not eliminate the truncation effect. We also found a smaller truncation effect in participants from quantitative fields when no warning was given, compared to participants from the humanities. However, this advantage was not evidence in the warning condition.

#### Figure 8

```{r figure 8 prep}
#Flat Violin Set Up

errbar_lims <- subject_mean_df %>% 
  group_by(field, graph_condition, subject_condition) %>% 
  summarise(mean=mean(subject_mean_rating), se=sd(subject_mean_rating)/sqrt(n()), 
                        upper=mean+(2*se), lower=mean-(2*se))

subject_mean_df %<>% 
  mutate(subject_condition_2 =ifelse(subject_condition == "no warning", 1, 2))

```


```{r figure 8}
#Flat Violin Pink and Blue

ggplot(subject_mean_df, aes(x = as.character(field), y = subject_mean_rating, fill = graph_condition)) +
  geom_flat_violin(
    position = position_nudge(x = .15, y = 0),
    adjust = 1.5,
    trim = FALSE,
    alpha = .7,
    colour = NA) +
  facet_grid(~subject_condition) +
  geom_point(
    aes(color = graph_condition),
    position = position_jitter(0.1),
    size = 2, shape = 20,  alpha =0.7) +
  geom_point(data=errbar_lims, 
    aes(x=field, y=mean, color = graph_condition),
    position = position_nudge(x = -0.28, y = 0)) +
  geom_errorbar(data=errbar_lims,
    aes(x=field, y=mean, ymax=upper, ymin=lower, color = graph_condition),  
    stat='identity', 
    size = 1, 
    width=.15, 
    position = position_nudge(x = -0.28, y = 0)) +
  #Color
  scale_colour_manual(values = c("#000A77", "#FF6171"))+
  scale_fill_manual(values = c("#000A77", "#FF6171"))+
  #Axis
  coord_cartesian(ylim=c(1, 7)) + 
  scale_y_continuous(breaks=seq(1, 7, 1))+
  ylab("mean rating")+
  xlab("field") +
  guides(fill = guide_legend(reverse=TRUE), #reverses color order
        color = guide_legend(reverse=TRUE)) +
  raincloud_theme +
  ggtitle("Figure 8") +
  geom_text(aes(label = "hello", x = Inf, y = 3), hjust = -1)
  

# ggsave('../figures/figure8.png', plot = last_plot(), width = 8, height = 5)
# ggsave('../figures/figure8.tiff', plot = last_plot(), width = 8, height = 5, device = "tiff")
```

### Effect of field: humanities versus quantitative

First, we ran a model comparison between the linear mixed effects model described above and a linear mixed effects model that did not include field and its associated interactions as predictors. The full model was statistically different from the model without field (p < .0001), confirming that the main effect (in addition to the simple effect reported above) of session was statistically significant.

```{r}

model_nofield <- lmer(rating ~ subject_condition + graph_condition + subject_condition:graph_condition + (1 | participantID), models_df)

anova(model_full, model_nofield)

```

To explore the interaction between field and warning condition, we computed a linear model predicting the size of the truncation effect-average ratings of truncated graphs minus the average ratings of control graphs-for each participant, with warning condition and field as binary predictors. We then calculated estimated marginal means (EMM) for humanities versus quantitative participants for the no warning and warning conditions respectively, averaging over graph types. We found that when no warning was given, PhD students from the humanities demonstrated a slightly larger truncation effect, compared to PhD students from quantitative fields: EMMhumanities = 0.69, SE = 0.06; EMMquantitative = 0.53, SE = 0.06; estimated difference = 0.17, SE = .08; t(326) = 2.00, p = .05. Notably, ratings between participants from humanities and quantitative fields did not differ when an explanatory warning was provided: t(326) = .49, p = .62.

```{r include=FALSE}
   
model_difference<- lm(difference ~ subject_condition*field, models_difference_df)

emm_fieldwarning = emmeans(model_difference, specs = pairwise ~ field|subject_condition, type = "response", lm.df = "satterthwaite")

```

Table 2
```{r echo=FALSE}

emm_fieldwarning$contrasts

```


```{r}
 emm_fieldwarning.df <-
    emm_fieldwarning$emmeans %>% broom::tidy()

  #plot marginal means
   emm_fieldwarning.df %>%
    ggplot(aes(subject_condition, estimate, ymin=conf.low, ymax=conf.high)) +
    geom_pointrange(aes(color = field), position = position_dodge(width = .25)) +
     #facet_grid(~field) +
     theme_minimal() +
     ylab("estimated size of the truncation effect") +
     xlab("") + 
     coord_cartesian(ylim=c(0, 1)) + #Axis
     scale_y_continuous(breaks=seq(0, 1, .1)) +
     ggtitle("Study 5. EMMs of the truncation effect")

```

We confirmed that the smaller truncation effect for quantitative PhD students was due to lower ratings for truncated graphs specifically (rather than, for example, higher ratings for control graphs) by calculating post-hoc contrasts and correcting for multiple comparisons with Tukey p-value adjustment for a family of 8 estimates. We found that humanities participants showed a larger truncation effect than quantitative participants (estimate = 0.36, SE = 0.10, p = .005) when no warning was given. This difference was not seen for control graphs in the no warning condition (p = .49).

  When given a warning, quant participants rate truncated graphs lower than humanities participants   (truncated,humanities,no warning - truncated,quantitative,no warning): estimate = 0.35876, SE =    0.0961, z.ratio = 3.732, p = 0.0047 
 
  This is not true for control graphs (control,humanities,no warning - control,quantitative,no 
  warning), estimate = 0.19155, SE = 0.0961, z.ratio = 1.993,  p = 0.4868.
  
```{r include=FALSE}

# estimated marginal means pair-wise comparisons for all combinations of IVs
emm1 = emmeans(model_full, specs = pairwise ~ graph_condition + field + subject_condition, lmer.df = "satterthwaite")

# estimated marginal means along with the SEs and CIs
# emm1$emmeans
  
  # make a tibble
  emm1.df <-
    emm1$emmeans %>% broom::tidy()

  #plot marginal means
   emm1.df %>%
    ggplot(aes(field, estimate, ymin=asymp.LCL, ymax=asymp.UCL)) +
    geom_pointrange(aes(color = graph_condition)) +
     facet_grid(~subject_condition) +
     theme_minimal() +
     ylab("estimated marginal means") +
     coord_cartesian(ylim=c(3, 5)) + #Axis
     scale_y_continuous(breaks=seq(3, 5, .5))
     study5_theme
   
```
  
```{r}
emm1$contrasts %>%
  broom::tidy()

```

Overall, we found that the truncation effect was persistent across all conditions. PhD students from quantitative fields exhibited a smaller truncation effect compared to PhD students from the humanities when no warning was given. However, when given a warning, the size of the truncation effect no longer varied by field. We note that even PhD students in quantitative fields given an explanatory warning about the nature of the manipulation immediately preceding graph ratings showed a truncation effect (estimated truncation effect = 0.24, SE = 0.06, p = .0008).

  Contrast: control,quantitative,warning - truncated,quantitative,warning
    estimate = -0.23554, SE = 0.0565, z.ratio =  -4.167, p = 0.0008 

```{r}

df <- subject_mean_df %>%
  filter(field == "quantitative") %>%
  filter(subject_condition == "warning")

t.test(subject_mean_rating ~ graph_condition, df, paired = TRUE)

```

# Supplemental Information

##  Methods (Supplemental Information)

### Participants (Supplemental Information)

```{r supplemental participant informatinon language}
langauge <- demographic_df %>% 
  group_by(dem_language) %>% 
  summarise(n = n())

other_language <- demographic_df %>% 
  filter(!is.na(dem_language_other)) %>% 
  group_by(dem_language_other) %>% 
  summarise(n = n())
  
```

`r text_spec(langauge[1,]$n, bold = T)` participants reported English as their first language, `r text_spec(sum(other_language$n), bold = T)` reported a different first language. Below is a list of all the languages that were reported.

```{r}
other_language
```


```{r supplemental participant information education}
education <- demographic_df %>% 
  group_by(dem_ed) %>% 
  summarise (n = n()) %>%
  mutate(percentage = n / sum(n)*100)

education
   
```

As expected `r text_spec(round(sum(education[education$dem_ed >= 6,]$percentage)), bold = T)`% of participants reported having at least a Bachelor’s degree and `r text_spec(round(sum(education[education$dem_ed >= 7,]$percentage)), bold = T)`% reported Completion of some graduate school courses or holding a Graduate degree. 

We recruited from PhD students from these schools:

```{r}
schools <- demographic_df %>% 
  group_by(dem_school) %>% 
  summarise(n())

schools
```


```{r}
schools_field <- demographic_df %>% 
  group_by(dem_school, field) %>% 
  summarise(n())

schools_field
```


```{r general participant information duration}
duration <- demographic_df %>% 
  group_by(field) %>% 
  summarise(mean_duration = mean(duration_min), 
            sd_duration = sd(duration_min), 
            median_duration = median(duration_min), 
            min_duration= min(duration_min), 
            max_duration = max(duration_min), 
            range_duration = max(duration_min)-min(duration_min))

duration
```

The experiment took participants from the humanities fields an average of M~duration~ = `r text_spec(round(duration[duration$field == "humanities",]$mean_duration, 2), bold = T)` minutes, (SD~duration~ = `r text_spec(round(duration[duration$field == "humanities",]$sd_duration, 2), bold = T)` minutes). 

The experiment took participants from the quantitative fields an average of M~duration~ = `r text_spec(round(duration[duration$field == "quantitative",]$mean_duration, 2), bold = T)` minutes, (SD~duration~ = `r text_spec(round(duration[duration$field == "quantitative",]$sd_duration, 2), bold = T)` minutes). 

### Statistical Training (Supplemental Information)

```{r}

#Code to have program
demographic_df$dem_department <- "other"
demographic_df[grep("econ", demographic_df$dem_program, ignore.case = TRUE),]$dem_department <- "economics"
demographic_df[grep("psy|neur", demographic_df$dem_program, ignore.case = TRUE),]$dem_department <- "psychology"
demographic_df[grep("stat", demographic_df$dem_program, ignore.case = TRUE),]$dem_department <- "statistics"
demographic_df[grep("eng|lit|writing|fiction", demographic_df$dem_program, ignore.case = TRUE),]$dem_department <- "english"
demographic_df[grep("his", demographic_df$dem_program, ignore.case = TRUE),]$dem_department <- "history"

training_program <- demographic_df %>% 
  group_by(field, dem_department) %>% 
  summarise(formal = sum(dem_stat_formal), informal = sum(dem_stat_informal), workshop = sum(dem_stat_workshop), other= sum(dem_stat_other), none = sum(dem_stat_none)) %>% 
  gather(training, counted, formal:none)

ggplot(training_program)+
  geom_bar(aes(x = dem_department, y = counted, fill = training), stat = "identity", position = "dodge")+
  facet_wrap(~field)

```


## Results (Supplemental Information)

#### Model Comparisons
This function as omnibus tests for main effects.

##### Warning Condition
The model including warning condition is statistically different than the model without field
```{r}

model_nowarningcondition <- lmer(rating ~ field+ graph_condition + field:graph_condition + (1 | participantID), models_df)

anova(model_full, model_nowarningcondition)

```

##### 3-way interaction
The model with the 3-way interaction is not statistically different from the model without the 3-way interaction (p =.27)
```{r}

model_no3way <- lmer(rating ~ subject_condition + graph_condition + field + subject_condition:graph_condition + field:graph_condition + field:subject_condition + (1 | participantID), models_df)

anova(model_full, model_no3way)

```

##### 2-way interactions

###### Evaluating graph:warning
Statistically significant (p < .0001)
```{r}

model_no3way <- lmer(rating ~ subject_condition + graph_condition + field + subject_condition:graph_condition + field:graph_condition + field:subject_condition + (1 | participantID), models_df)

model_nographwarning <- lmer(rating ~ subject_condition + graph_condition + field + field:graph_condition + field:subject_condition + (1 | participantID), models_df)

anova(model_no3way, model_nographwarning)

```

###### Evaluating field:graph type
Not statistically different at alpha = .05 (p = .065)
```{r}

model_no3way <- lmer(rating ~ subject_condition + graph_condition + field + subject_condition:graph_condition + field:graph_condition + field:subject_condition + (1 | participantID), models_df)

model_nographfield <- lmer(rating ~ subject_condition + graph_condition + field + subject_condition:graph_condition + field:subject_condition + (1 | participantID), models_df)

anova(model_no3way, model_nographfield)

```

###### Evaluating field:warning
Statistically different at p = .003
```{r}

model_no3way <- lmer(rating ~ subject_condition + graph_condition + field + subject_condition:graph_condition + field:graph_condition + field:subject_condition + (1 | participantID), models_df)

model_nofieldwarning <- lmer(rating ~ subject_condition + graph_condition + field + subject_condition:graph_condition + field:graph_condition + (1 | participantID), models_df)

anova(model_no3way, model_nofieldwarning)

```

#### Graph literacy

##### Main Effect
Graph literacy does not explain the size of the truncation effect:
  
```{r graph literacy model}
graphliteracymodel <- lm(difference ~ graphliteracy_sum_rating, models_difference_df)
summary_graphliteracymodel <- summary(graphliteracymodel)
summary_graphliteracymodel
```


```{r graph literacy model plot}
ggplot(models_difference_df, aes(x = graphliteracy_sum_rating, y = difference))+
  geom_smooth(method = lm, color = "#510D73", fill = "#510D73") +
  geom_jitter(alpha = 0.7)+
  ylab("truncation effect")+
  xlab("graph literacy score")+
  # ggtitle("Figure SI5 Graph Literacy")+
  raincloud_theme+
  theme(axis.line.x = element_line(colour = 'black', size=0.5, linetype='solid'))

ggsave('../figures/figureSI5_graphliteracy.png', plot = last_plot(), width = 6, height = 4)
```


##### Interaction with warning and field

```{r graph literacy model complete}
graphliteracymodel_complete <- lm(difference ~ graphliteracy_sum_rating * field * subject_condition, models_difference_df)
summary_graphliteracymodel_complete <- summary(graphliteracymodel_complete)
summary_graphliteracymodel_complete
```


```{r graph literacy model complete plot}
ggplot(models_difference_df, aes(x = graphliteracy_sum_rating, y = difference, color = field))+
  geom_smooth(method = lm) +
  geom_jitter(alpha = 0.7)+
  ylab("truncation effect")+
  xlab("graph literacy score")+
  raincloud_theme+
  theme(axis.line.x = element_line(colour = 'black', size=0.5, linetype='solid'))

# ggsave('../figures/figureSI5_graphliteracy.png', plot = last_plot(), width = 6, height = 4)
```

#### Age (Supplemental Information)

```{r}

df <- left_join(subject_difference_df, demographic_df, by = "participantID")

# age

summary(
  lm(difference ~ dem_age, df)
)

summary(
  lm(difference ~ dem_age + field.x + subject_condition.x, df)
)


```


#### Timing (Supplemental Information)

```{r supplemental information timing}
Trimming <- TRUE
if(Trimming == TRUE){timing_t_df = trimmed_timing_df
} else{timing_t_df = timing_df}
```


Timing information is in seconds. 

You can choose to trim or not timing data. By default any timing that is 2 standard deviations away from the mean (for each participant for each condition) is trimmed. 

For this report, any timing that is 2 standard deviations away from the mean (for each participant for each condition) `r if(Trimming == TRUE){paste("WAS")} else{paste("WAS NOT")}` trimmed.  If you want to see results when participants who got the exercise wrong are excluded/included, you can go to the section called Exclusions (at the top of this file) and change **Trimming <- FALSE**
  
  
```{r timing interaction}
subject_timing <-  timing_t_df %>%
  group_by(participantID, graph_condition, subject_condition, field) %>%
  summarise(subject_mean_timing = mean(time))


#Data Summary
subject_timing_interaction_summary <- subject_timing %>% 
  group_by(field, subject_condition, graph_condition) %>% 
  summarise(n = n(), mean = mean(subject_mean_timing), 
            sd = sd(subject_mean_timing), 
            median = median(subject_mean_timing), min= min(subject_mean_timing), 
            max = max(subject_mean_timing), 
            range = max(subject_mean_timing)-min(subject_mean_timing))

subject_timing_interaction_summary
```

```{r timing by graph condition}
#Data Summary
subject_timing_graph_summary <- subject_timing %>% 
  group_by(graph_condition) %>% 
  summarise(n = n(), mean = mean(subject_mean_timing), 
            sd = sd(subject_mean_timing), 
            median = median(subject_mean_timing), min= min(subject_mean_timing), 
            max = max(subject_mean_timing), 
            range = max(subject_mean_timing)-min(subject_mean_timing))

subject_timing_graph_summary

```


```{r timing by warning condition}
#Data Summary
subject_timing_warning_summary <- subject_timing %>% 
  group_by(subject_condition) %>% 
  summarise(n = n()/2, mean = mean(subject_mean_timing), 
            sd = sd(subject_mean_timing), 
            median = median(subject_mean_timing), min= min(subject_mean_timing), 
            max = max(subject_mean_timing), 
            range = max(subject_mean_timing)-min(subject_mean_timing))

subject_timing_warning_summary
```

```{r timing by field, eval=FALSE, include=FALSE}
#Data Summary
subject_timing_field_summary <- subject_timing %>% 
  group_by(field) %>% 
  summarise(n = n()/2, mean = mean(subject_mean_timing), 
            sd = sd(subject_mean_timing), 
            median = median(subject_mean_timing), min= min(subject_mean_timing), 
            max = max(subject_mean_timing), 
            range = max(subject_mean_timing)-min(subject_mean_timing))

subject_timing_field_summary
```

# Previous Results
These results were reported in the pre-print and older versions of the manuscript.

## Effect of field: humanities versus quantitative

We computed models for each warning condition separately. In these two models, graph type (0 = control, 1 = truncated) and field (0 = humanities, 1 = quantitative) served as binary fixed factors, with graph ratings as the outcome variable and participants included as a random effect . Table 3 shows these model parameters.

### No Warning
```{r mixed effect model no warning, eval=FALSE, include=FALSE}
no_warning_df <-  models_df %>% 
  filter(subject_condition == "no warning")
  
model_no_warning <- lmer(rating ~  graph_condition * field + (1 | participantID), no_warning_df)
summary_model_no_warning <- summary(model_no_warning)
summary_model_no_warning <- as.data.frame(summary_model_no_warning$coefficients)


##Confidence Intervals
ci_model_no_warning <- confint(model_no_warning, "beta_", level = 0.95, method = c("boot"), nsim = 1000,  seed = 833, boot.type = c("perc"), FUN=NULL, quiet=FALSE, oldNames = TRUE)
ci_model_no_warning <- as.data.frame(ci_model_no_warning)


## Create and Clean Table
table3 <- merge(summary_model_no_warning, ci_model_no_warning, by = "row.names")
numVars <- sapply(table3, is.numeric) 
table3[numVars] <- lapply(table3[numVars], round, digits = 2) 

table3 %<>% 
  mutate("95% CI of Estimate" =  paste("[", `2.5 %`, ",", `97.5 %`, "]", sep = "")) %>% select(Row.names, Estimate, `95% CI of Estimate`, `Std. Error`, `t value`, `Pr(>|t|)`)

table3
```

### Warning
```{r mixed effect model warning, eval=FALSE, include=FALSE}
warning_df <-  models_df %>% 
  filter(subject_condition == "warning")
  
model_warning <- lmer(rating ~  graph_condition * field + (1 | participantID), warning_df)
summary_model_warning <- summary(model_warning)
summary_model_warning <- as.data.frame(summary_model_warning$coefficients)


##Confidence Intervals
ci_model_warning <- confint(model_warning, "beta_", level = 0.95, method = c("boot"), nsim = 1000,  seed = 833, boot.type = c("perc"), FUN=NULL, quiet=FALSE, oldNames = TRUE)
ci_model_warning <- as.data.frame(ci_model_warning)


## Create and Clean Table
table3_2 <- merge(summary_model_warning, ci_model_warning, by = "row.names")
numVars <- sapply(table3_2, is.numeric) 
table3_2[numVars] <- lapply(table3_2[numVars], round, digits = 2) 

table3_2 %<>% 
  mutate("95% CI of Estimate" =  paste("[", `2.5 %`, ",", `97.5 %`, "]", sep = "")) %>% select(Row.names, Estimate, `95% CI of Estimate`, `Std. Error`, `t value`, `Pr(>|t|)`)

table3_2

```


```{r eval=FALSE, include=FALSE}
When a warning was given, we did not find effects that were qualified by field. In other words, the advantage for quantitative fields fades with a warning. This was revealed by a statistically significant effect of graph type ( b = `r text_spec(round(table3_2[3,]$Estimate, 2), bold = T)`, SE = `r text_spec(round(table3_2[3,]$"Std. Error", 2), bold = T)`; Table 4). We did not find statistically significant effects of field or an interaction between graph type or field. This difference from the no warning condition was primarily driven by participants in the humanities condition exhibiting smaller truncation effects, rather than participants in the quantitative condition exhibiting larger truncation effects, as illustrated in Figure 6. 
```

```{r mixed effects model S}
model1<- lmer(rating ~ subject_condition*graph_condition + (1 | participantID), models_df)
summary_model1 <- summary(model1)
summary_model1 <- as.data.frame(summary_model1$coefficients)
```
